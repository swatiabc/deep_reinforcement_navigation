{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_size:  4\n",
      "action:  2 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  1.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  0.0 Reward:  -1.0 done:  False\n",
      "action:  0 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  0 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  1 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  0.0 Reward:  0.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  1.0 done:  False\n",
      "action:  3 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  False\n",
      "action:  2 score:  1.0 Reward:  0.0 done:  True\n",
      "Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]  # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "print(\"action_size: \",action_size)\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state   \n",
    "    print(\"action: \",action, \"score: \",score, \"Reward: \",reward, \"done: \",done)      # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: box2d in /usr/local/anaconda3/envs/drlnd/lib/python3.6/site-packages (2.3.10)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "!pip3 install box2d\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "    \n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from dq_agent import Agent\n",
    "agent = Agent(state_size=37, action_size=4, seed=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.83\n",
      "Episode 200\tAverage Score: 5.05\n",
      "Episode 300\tAverage Score: 8.29\n",
      "Episode 400\tAverage Score: 11.29\n",
      "Episode 500\tAverage Score: 12.82\n",
      "Episode 600\tAverage Score: 13.76\n",
      "Episode 700\tAverage Score: 14.17\n",
      "Episode 800\tAverage Score: 15.29\n",
      "Episode 900\tAverage Score: 15.34\n",
      "Episode 1000\tAverage Score: 15.63\n",
      "Episode 1100\tAverage Score: 16.28\n",
      "Episode 1200\tAverage Score: 16.54\n",
      "Episode 1300\tAverage Score: 15.98\n",
      "Episode 1400\tAverage Score: 15.66\n",
      "Episode 1500\tAverage Score: 16.13\n",
      "Episode 1600\tAverage Score: 15.57\n",
      "Episode 1700\tAverage Score: 15.28\n",
      "Episode 1800\tAverage Score: 16.10\n",
      "Episode 1900\tAverage Score: 15.66\n",
      "Episode 2000\tAverage Score: 16.09\n"
     ]
    }
   ],
   "source": [
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]  # reset the environment\n",
    "        state = env_info.vector_observations[0]            # get the current state\n",
    "        score = 0                                          # initialize the score\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)                 # select an action\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            score += reward                                # update the score\n",
    "            state = next_state\n",
    "            # print(\"i_episode: \",i_episode, \"t: \",t, \"score: \",score, \"reward: \",reward)\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=20:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores = dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('fc1.weight', tensor([[-2.6277e-01, -9.3241e-03, -1.7641e-01,  ..., -5.9304e-01,\n",
      "         -4.0480e-01,  8.3685e-03],\n",
      "        [ 1.3780e-01,  7.5200e-02,  8.7206e-02,  ...,  1.7406e-01,\n",
      "          1.7419e-02, -1.0440e-02],\n",
      "        [-2.8536e-02,  1.5977e-01, -2.5497e-02,  ..., -1.1370e-02,\n",
      "          1.6889e-01,  3.7207e-02],\n",
      "        ...,\n",
      "        [-6.5790e-02,  6.4234e-02, -1.7804e-01,  ...,  1.7944e-01,\n",
      "         -2.8715e-01, -5.4399e-02],\n",
      "        [-1.0266e-01,  2.4883e-01, -3.8296e-01,  ..., -1.9787e-02,\n",
      "         -1.0504e-01, -5.4533e-02],\n",
      "        [-4.5417e-02, -4.3452e-02,  9.8037e-02,  ...,  6.6270e-02,\n",
      "          2.5606e-02,  7.6410e-04]])), ('fc1.bias', tensor([ 0.1032, -0.0390, -0.1211,  0.1619, -0.0299, -0.2496, -0.1461,\n",
      "         0.1056, -0.0731,  0.0098, -0.1994, -0.1404, -0.1023, -0.0531,\n",
      "         0.2095,  0.1383, -0.1777,  0.0327,  0.0004, -0.0358, -0.0862,\n",
      "         0.0327, -0.0258,  0.0050, -0.1991,  0.0924, -0.0810,  0.1013,\n",
      "         0.0865, -0.0943, -0.0141,  0.2954,  0.3024, -0.1001,  0.0044,\n",
      "        -0.1767, -0.0087, -0.0375, -0.0897, -0.0045, -0.2079,  0.1825,\n",
      "        -0.0644,  0.1001,  0.0464,  0.1544, -0.0047,  0.0165,  0.0001,\n",
      "        -0.3565,  0.0958, -0.0553,  0.0890,  0.1415,  0.1638,  0.0179,\n",
      "         0.0220,  0.0652, -0.0274,  0.0832, -0.1956, -0.2154, -0.2089,\n",
      "        -0.0109])), ('fc2.weight', tensor([[ 2.4650e-01, -1.4861e-01,  8.2748e-02,  ...,  5.8174e-02,\n",
      "         -1.3160e-02, -7.4284e-02],\n",
      "        [-1.2674e-01, -3.1047e-01, -1.8173e-01,  ..., -2.4142e-01,\n",
      "         -1.7055e-01,  8.3141e-02],\n",
      "        [-2.5664e-01, -1.1289e-01,  1.1255e-01,  ..., -1.0300e-01,\n",
      "         -9.3735e-01,  4.4990e-02],\n",
      "        ...,\n",
      "        [-1.0369e-01,  1.7135e-01,  1.7154e-01,  ...,  9.8005e-02,\n",
      "         -8.8648e-02,  1.0157e-01],\n",
      "        [ 1.9149e-01,  2.9335e-01, -1.2073e-02,  ..., -8.5091e-02,\n",
      "         -2.2297e-01,  1.5463e-02],\n",
      "        [ 5.0820e-01,  3.4187e-02,  7.2158e-02,  ...,  1.3511e-01,\n",
      "         -3.9653e-01, -4.6254e-03]])), ('fc2.bias', tensor([ 0.0517, -0.0405,  0.0021, -0.0978, -0.0514,  0.3411, -0.0509,\n",
      "         0.0719, -0.0084,  0.1800,  0.1718, -0.1752, -0.3212,  0.1911,\n",
      "         0.3747,  0.1180, -0.2742,  0.1179,  0.0099,  0.1055,  0.4131,\n",
      "         0.2300,  0.6007, -0.1873,  0.2712,  0.0934, -0.1960, -0.0055,\n",
      "         0.4514, -0.0204, -0.1253,  0.9033,  0.0495,  0.0643,  0.0132,\n",
      "         0.2673, -0.1269,  0.1179,  0.9316,  0.5976, -0.2727,  0.6383,\n",
      "         0.8941, -0.2118,  0.4200,  0.4092,  0.0635, -0.0256, -0.1770,\n",
      "         0.2293, -0.2635,  0.8746, -0.2703, -0.1102,  0.5439, -0.1256,\n",
      "         0.1857, -0.1249, -0.1921,  0.2453, -0.2296, -0.1924, -0.2613,\n",
      "         0.3382])), ('fc3.weight', tensor([[ 0.2994, -0.7758, -1.1096, -0.0906,  0.0059,  0.1371,  0.6332,\n",
      "          0.2918, -0.6424,  0.1273,  0.2550,  0.1174, -0.3650,  0.2377,\n",
      "          0.1411,  0.2801, -0.1572,  0.1435,  0.2177,  0.0542,  0.0891,\n",
      "          0.1591,  0.1943, -0.1523,  0.2380, -0.2184, -0.0856,  0.3127,\n",
      "          0.2344, -0.1441, -0.2236,  0.2875,  0.3962,  0.2990, -0.0249,\n",
      "          0.1686, -0.1755,  0.2604,  0.3170,  0.1599, -0.0408,  0.2225,\n",
      "          0.3156, -0.6953,  0.2185,  0.2793,  0.0733, -0.2425, -0.1685,\n",
      "          0.1927, -0.0761,  0.2453, -0.2357, -0.2265,  0.2378,  0.0352,\n",
      "          0.1813,  0.1344, -0.2695,  0.4135, -0.2259, -0.1387, -0.1993,\n",
      "          0.0887],\n",
      "        [ 0.5617, -0.0573, -0.1207, -0.0076,  0.0710,  0.1461,  0.6276,\n",
      "          0.3685, -0.4801, -0.0120,  0.1598,  0.1373, -0.3340,  0.3195,\n",
      "          0.2368,  0.1885, -0.0966,  0.1898, -0.0629,  0.0066,  0.1086,\n",
      "          0.1314,  0.2843,  0.0140,  0.2205,  0.0903,  0.0945,  0.6548,\n",
      "          0.1711, -0.0341, -0.0067,  0.3108,  0.3881,  0.3192,  0.0609,\n",
      "          0.1382,  0.0840,  0.2582,  0.2673,  0.0642, -0.0716,  0.1626,\n",
      "          0.2889, -0.1173,  0.2269,  0.1917,  0.1198,  0.0319,  0.1558,\n",
      "          0.1925, -0.2275,  0.3174, -0.0763,  0.0275,  0.2061,  0.0211,\n",
      "          0.2337,  0.2720,  0.0413,  0.2081, -0.3200,  0.0956, -0.0139,\n",
      "          0.2019],\n",
      "        [ 0.2222,  0.0921, -0.3841, -0.0594,  0.2002,  0.1842,  0.7073,\n",
      "          0.2601, -0.3119,  0.1764,  0.0560,  0.4492, -0.1043, -0.0323,\n",
      "          0.2234,  0.2653, -0.4050,  0.0875,  0.2635,  0.3139,  0.0181,\n",
      "         -0.1565,  0.3892, -0.0701,  0.0478,  0.2215, -0.0317,  0.0498,\n",
      "          0.1136,  0.1649, -0.0377,  0.3011,  0.4586,  0.1971,  0.0441,\n",
      "          0.2175, -0.2235,  0.0315,  0.3143,  0.1527,  0.1536,  0.2335,\n",
      "          0.2643, -0.1018,  0.2990,  0.1944,  0.2169, -0.4034,  0.3533,\n",
      "         -0.1141, -0.0220,  0.2216, -0.5199,  0.0844,  0.3241, -0.0748,\n",
      "          0.2592, -0.0305,  0.2041,  0.3461, -0.2421,  0.0677,  0.2213,\n",
      "          0.0038],\n",
      "        [ 0.2685,  0.4478, -0.2976, -0.0786,  0.2269,  0.3141,  0.0393,\n",
      "         -0.0306, -0.6091,  0.2555,  0.3087,  0.0363, -0.1059,  0.1398,\n",
      "          0.2820,  0.0548, -0.4353, -0.3361,  0.0316,  0.1883,  0.1521,\n",
      "         -0.1375,  0.2248, -0.1411,  0.1777,  0.2449, -0.2837,  0.8657,\n",
      "          0.1195,  0.0465, -0.0911,  0.2746,  0.2582,  0.2038,  0.2240,\n",
      "         -0.1113, -0.5231,  0.2733,  0.2277,  0.2036,  0.3138,  0.2666,\n",
      "          0.3699, -0.1972,  0.0883,  0.2381, -0.2721, -0.1838, -0.3934,\n",
      "         -0.0710, -0.3328,  0.3104, -0.1736,  0.2112,  0.3207,  0.0165,\n",
      "         -0.1408, -0.3093,  0.1417,  0.2857,  0.0338,  0.3925,  0.1924,\n",
      "          0.2271]])), ('fc3.bias', tensor([ 0.6772,  0.4957,  0.5557,  0.5064]))])\n"
     ]
    }
   ],
   "source": [
    "print(agent.qnetwork_local.state_dict())\n",
    "torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 21.0\n"
     ]
    }
   ],
   "source": [
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:      \n",
    "    action = agent.act(state)                      # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
